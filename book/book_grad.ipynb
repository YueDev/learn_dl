{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd949483bb4959f",
   "metadata": {},
   "source": [
    "## 求导和梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab69081b5c92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4b67c22060109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数fx在x点上的导数，这里用中心差分\n",
    "def numerical_diff(f, x):\n",
    "    # 偏差不能太小 计算机由于精度原因会把太小的值当作0\n",
    "    delta_x = 1e-4\n",
    "    # 由于delta_x不能无限小，因此用中心差分来减少误差\n",
    "    # dfx/dx = (f(x + delta) - f (x - delta)) / 2h\n",
    "    return (f(x + delta_x) - f(x - delta_x)) / (2 * delta_x)\n",
    "\n",
    "\n",
    "f = lambda x: x ** 2 + 3 * x + 100\n",
    "numerical_diff(f, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a219852cb4499",
   "metadata": {},
   "source": [
    "标量y对向量求导,即为该向量的梯度， 应该需要转置，即行向量的梯度为列向量\n",
    "\n",
    "但是书上没有转制，大概后续乘法会交换顺序把。\n",
    "\n",
    "这里按照书上来，先不转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c240377a7b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x是向量\n",
    "def numerical_gradient(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    # 遍历x，分别对每一个参数求导，\n",
    "    # 实际上就是对当前的x[i]进行中心分差计算，其他元素不变\n",
    "    for i in range(x.size):\n",
    "        temp_x_i = x[i]\n",
    "        x[i] = temp_x_i + delta_x\n",
    "        fx1 = f(x)\n",
    "        x[i] = temp_x_i - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[i] = (fx1 - fx2) / (2 * delta_x)\n",
    "        x[i] = temp_x_i\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "# 测试梯度函数\n",
    "f = lambda a: a[0] ** 2 + 2 * a[0] * a[1] + a[2] ** 2\n",
    "numerical_gradient(f, np.array([3.0, 4.0, 5.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbfde840631fe3",
   "metadata": {},
   "source": [
    "接下来可以写出梯度下降法的函数了:x = x - lr * grad_x  \n",
    "x是向量，其中lr是学习率，grad_x是x的梯度  \n",
    "分解开就是:\n",
    "    x[0] = x[0] - lr * grad_x[0]    \n",
    "    x[1] = x[1] - lr * grad_x[1]\n",
    "    ... \n",
    "    x[n] = x[n] - lr * grad_x[n]  \n",
    "重复一定次数即可，求出fx的极小值/最小值了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d542e5f3c77fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降求函数的fx极小值的x坐标，lr系数，学习率 step重复次数\n",
    "def gradient_descent(f, init_x, lr=0.01, step=100):\n",
    "    x = init_x\n",
    "    for i in range(step):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x = x - lr * grad\n",
    "    return x\n",
    "\n",
    "\n",
    "# 测试下梯度下降\n",
    "fx = lambda x: x[0] ** 2 + x[1] ** 2\n",
    "init_x = np.array([33.0, 20.0])\n",
    "gradient_descent(fx, init_x, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c47cef",
   "metadata": {},
   "source": [
    "定义一个简单的单层网络类，权重w是一个2 * 3的矩阵，偏置先不设置，激活函数是softmax\n",
    "\n",
    "可以计算出这个网络的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72117aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_a = np.exp(x - c)  # 减去c是为了防止溢出\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "\n",
    "# 损失函数 交叉熵误差\n",
    "def cross_entropy_error(y, t):\n",
    "    # 由于loge(0)是负无穷大-inf，计算机无法继续之后的运算\n",
    "    # 所以给输入增加一个微小的数，并且不影响结果\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "class SimpleNet:\n",
    "\n",
    "    # 初始化网络    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "    # 推理一次 即forward    \n",
    "    def predict(self, x):\n",
    "        y = np.dot(x, self.w)\n",
    "        y = softmax(y)\n",
    "        return y\n",
    "\n",
    "    # 网络的损失\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9457c6cd7741770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置np的打印选项：精度最大为小数点后4位，不用科学计数法\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# 简单测试下网络\n",
    "net = SimpleNet()\n",
    "x = np.array([[0.6, 0.9]])\n",
    "t = np.array([0.0, 0.0, 1.0])\n",
    "d = net.loss(x, t)\n",
    "print('loss:', d)\n",
    "\n",
    "\n",
    "# 一个通用的算术求梯度，遍历ndarray进行运算\n",
    "def numerical_gradient_common(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    # 利用迭代器遍历ndarray，这样才能保证拿到索引并且有修改ndarray的权限\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    for _ in it:  # 也可以这么写while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        temp = x[idx]\n",
    "        x[idx] = temp + h\n",
    "        fx1 = f(x)\n",
    "        x[idx] = temp - h\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * h)\n",
    "        x[idx] = temp\n",
    "    return grad\n",
    "\n",
    "\n",
    "# loss函数输入x t，再通过与w b的计算 可以表示为y = f(w) + f(b)\n",
    "# 因此这里损失对w求导，直接用loss_w即可，求导函数里每次调用f(x)的时候，会按照当前w b x t计算一次\n",
    "# 而我们传入的net.w，进行了微小的更改+h和-h，这里就是损失对w求偏导了\n",
    "# 可以测试一下\n",
    "loss_w = lambda w: net.loss(x, t)\n",
    "grad = numerical_gradient_common(loss_w, net.w)\n",
    "\n",
    "print('grad:', grad)\n",
    "\n",
    "# w[0][0]增加1 看对应的loss增加多少\n",
    "net.w[0][0] += 1\n",
    "d = net.loss(x, t)\n",
    "print('loss:', d)\n",
    "# loss应该是差不多增加了grad[0][0] * w[0][0],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff08242",
   "metadata": {},
   "source": [
    "接下来测试下sgd随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7132be13c578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给一组真实数据\n",
    "true_x = np.array([[0.2, 0.3]])\n",
    "true_w = np.array([[0.3, -0.5, 1.2], [0.2, 1.2, 0.5]])\n",
    "true_y = true_x.dot(true_w)\n",
    "true_y = softmax(true_y)\n",
    "max = np.argmax(true_y)\n",
    "true_y = np.zeros_like(true_y)\n",
    "true_y[0][max] = 1\n",
    "\n",
    "nn = SimpleNet()\n",
    "d = nn.loss(true_x, true_y)\n",
    "print('loss 1:', d)\n",
    "loss_nn = lambda _: nn.loss(true_x, true_y)\n",
    "\n",
    "# sgd \n",
    "lr = 0.1\n",
    "step = 1000\n",
    "for _ in range(step):\n",
    "    grad = numerical_gradient_common(loss_nn, nn.w)\n",
    "    nn.w -= lr * grad\n",
    "\n",
    "# 打印sgd之后的误差 和推理结果\n",
    "d = nn.loss(true_x, true_y)\n",
    "print('loss 2:', d)\n",
    "y_hat = nn.predict(true_x)\n",
    "print('y_hat:', y_hat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c9d5c",
   "metadata": {},
   "source": [
    "现在正常模拟一个完整的网络，数据用的是d2l_08里的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3ac360d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:01:31.498677Z",
     "start_time": "2023-10-17T02:01:31.492873Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成模拟数据  \n",
    "def synthetic_data(w, num_examples):\n",
    "    X = np.random.randn(num_examples, len(w))\n",
    "    y = X.dot(w)\n",
    "    # 为了使数据更真实，加上一个随机干扰    \n",
    "    y += np.random.normal(0, 0.01, y.shape)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# w是列向量\n",
    "true_w = np.array([8.0, -6.2]).reshape(-1, 1)\n",
    "features, labels = synthetic_data(true_w, 1000)\n",
    "\n",
    "\n",
    "# 批量样本的生成器 每次调用next(g)，都会得到batch_size个数据\n",
    "def data_iter(batch_size, features, labels):\n",
    "    # 随机下样本索引     \n",
    "    num_examples = len(features)\n",
    "    indices = np.arange(num_examples)\n",
    "    indices = np.random.permutation(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        start = i\n",
    "        end = min(i + batch_size, num_examples)\n",
    "        select_indices = indices[start:end]\n",
    "        yield features[select_indices], labels[select_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fe013de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T01:57:46.315915Z",
     "start_time": "2023-10-17T01:57:46.307836Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义网络相关的参数 不定义类了\n",
    "\n",
    "n_w = np.random.normal(0, 0.01, (2, 1))\n",
    "# 学习率\n",
    "lr = 0.1\n",
    "# 每一个batch的大小\n",
    "batch_size = 100\n",
    "# epochs次，即总共学习3次\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "# 均方误差\n",
    "def square_loss(y, t):\n",
    "    return (y - t) ** 2 / 2\n",
    "\n",
    "\n",
    "# 推理一次，由于用了均方误差，输出函数用恒等函数即可。\n",
    "def predict(X):\n",
    "    return X.dot(n_w)\n",
    "\n",
    "# 运行网络并计算网络的损失\n",
    "# 由于X t是批量，因此要返回均值作为网络的误差\n",
    "def loss_net(X, t):\n",
    "    y = predict(X)\n",
    "    l = square_loss(y, t)\n",
    "    return l.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b7a10275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:01:36.508639Z",
     "start_time": "2023-10-17T02:01:36.498197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.880357672359419\n",
      "0.6555274947032093\n",
      "0.07381425778606433\n",
      "0.008416755166289548\n",
      "0.0010107927298334328\n",
      "[[-0.0398]\n",
      " [ 0.0178]]\n"
     ]
    }
   ],
   "source": [
    "n_w = np.random.normal(0, 0.01, (2, 1))\n",
    "\n",
    "# 开始学习 一共学习num_epochs轮\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    for X, t in data_iter(batch_size, features, labels):\n",
    "        # 运行网络并计算损失\n",
    "        loss_grad = lambda _ : loss_net(X, t)\n",
    "        # 计算梯度\n",
    "        grad = numerical_gradient_common(loss_grad, n_w)\n",
    "        # 修改参数\n",
    "        n_w -= lr * grad\n",
    "        \n",
    "    # 学习完一轮， 计算精度\n",
    "    l = loss_net(features, labels)\n",
    "    print(l)\n",
    "\n",
    "#最后打印n_w 与true_w的差\n",
    "print(n_w - true_w)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9cc2a60f9d33947"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
