{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd478bc9",
   "metadata": {},
   "source": [
    "## 比较计算图与算数求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本的数量\n",
    "feature_num = 1000\n",
    "# 单个样本的长度\n",
    "feature_length = 5\n",
    "# 标签的长度\n",
    "label_length = 3\n",
    "\n",
    "features = np.random.randn(feature_num, feature_length)\n",
    "\n",
    "labels = np.zeros((feature_num, label_length))\n",
    "\n",
    "max_index = features.argmax(axis=1) % label_length\n",
    "\n",
    "row_index = np.arange(feature_num)\n",
    "labels[row_index, max_index] = 1.0\n",
    "    \n",
    "\n",
    "\n",
    "#批量的样本生成器\n",
    "def data_iter(batch_size, features, labels):\n",
    "    # 随机下样本索引     \n",
    "    num_examples = len(features)\n",
    "    indices = np.arange(num_examples)\n",
    "    indices = np.random.permutation(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        start = i\n",
    "        end = min(i + batch_size, num_examples)\n",
    "        select_indices = indices[start:end]\n",
    "        yield features[select_indices], labels[select_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def relu(x):\n",
    "    mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[mask] = 0\n",
    "    return out\n",
    "\n",
    "# 输出函数\n",
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(1, -1)\n",
    "    c = np.max(x, axis=1, keepdims=True)\n",
    "    exp_a = np.exp(x - c)  # 减去c是为了防止溢出\n",
    "    sum_exp_a = np.sum(exp_a, axis=1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "\n",
    "# 损失函数 \n",
    "def cross_entropy_error(y, t):\n",
    "    # 由于loge(0)是负无穷大-inf，计算机无法继续之后的运算\n",
    "    # 所以给输入增加一个微小的数，并且不影响结果\n",
    "    delta = 1e-7\n",
    "    # 除以批量\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "\n",
    "# 求梯度，遍历ndarray进行运算\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    # 利用迭代器遍历ndarray，这样才能保证拿到索引并且有修改ndarray的权限\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    num = 0\n",
    "    for _ in it:  # 也可以这么写while not it.finished:\n",
    "        num += 1\n",
    "        idx = it.multi_index\n",
    "        temp = x[idx]\n",
    "        x[idx] = temp + h\n",
    "        fx1 = f(x)\n",
    "        x[idx] = temp - h\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * h)\n",
    "        x[idx] = temp\n",
    "    return grad\n",
    "\n",
    "\n",
    "# 计算图的各种层\n",
    "class Relu:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        dout = x.dot(self.W) + self.b\n",
    "        return dout\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    # 输出函数 批量操作\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        c = np.max(x, axis = 1, keepdims = True)\n",
    "        exp_a = np.exp(x - c) # 减去c是为了防止溢出\n",
    "        sum_exp_a = np.sum(exp_a, axis = 1, keepdims = True)\n",
    "        y = exp_a / sum_exp_a\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.y = self.softmax(x)\n",
    "        self.t = t\n",
    "        loss = cross_entropy_error(self.y, t)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        dx = (self.y -self.t)\n",
    "        return dx\n",
    "\n",
    "\n",
    "# 定义网络\n",
    "# 两层网络 第一层是affine层 \n",
    "# 激活函数Sigmoid\n",
    "# 两层网络 第二层是affine层\n",
    "# 输出函数是softmax 误差是交叉熵误差\n",
    "class SimpleNet:\n",
    "\n",
    "    def __init__(self, l1_size, l2_size, out_size):\n",
    "        \n",
    "        self.params = {\n",
    "            'W1': np.random.rand(l1_size, l2_size),\n",
    "            'B1': np.zeros(l2_size),\n",
    "            'W2': np.random.rand(l2_size, out_size),\n",
    "            'B2': np.zeros(out_size),\n",
    "        }\n",
    "        \n",
    "        self.layer_params = {\n",
    "            'W1': self.params['W1'].copy(), \n",
    "            'B1': self.params['B1'].copy(),\n",
    "            'W2': self.params['W2'].copy(),\n",
    "            'B2': self.params['B2'].copy(),\n",
    "        }\n",
    "\n",
    "        # 生成层\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.layer_params['W1'],self.layer_params['B1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.layer_params['W2'], self.layer_params['B2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        w1, w2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['B1'], self.params['B2']\n",
    "\n",
    "        a1 = x.dot(w1) + b1\n",
    "        z1 = relu(a1)\n",
    "\n",
    "        a2 = z1.dot(w2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \n",
    "        f_loss = lambda _: self.loss(x, t)\n",
    "        grads = {\n",
    "            'W1': numerical_gradient(f_loss, self.params['W1']),\n",
    "            'B1': numerical_gradient(f_loss, self.params['B1']),\n",
    "            'W2': numerical_gradient(f_loss, self.params['W2']),\n",
    "            'B2': numerical_gradient(f_loss, self.params['B2']),\n",
    "        }\n",
    "        return grads\n",
    "\n",
    "\n",
    "    # 求精度\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    # 以下是计算图关的\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return self.lastLayer.forward(x, t)\n",
    "    \n",
    "    def backward(self, x, t):\n",
    "        self.forward(x, t)\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.layers['Affine1'].dW, \n",
    "            'B1': self.layers['Affine1'].db, \n",
    "            'W2': self.layers['Affine2'].dW,\n",
    "            'B2': self.layers['Affine2'].db, \n",
    "        }\n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#测试一下两种求梯度的方法\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "\n",
    "layer1_size = feature_length\n",
    "layer2_size = 4\n",
    "out_size = label_length\n",
    "\n",
    "net = SimpleNet(l1_size=layer1_size, l2_size=layer2_size, out_size=out_size)\n",
    "index = np.random.randint(feature_length)\n",
    "x = features[index]\n",
    "t = labels[index]\n",
    "grad = net.gradient(x, t)\n",
    "# TODO 这里反向传播的计算有问题\n",
    "grad_layer = net.backward(x, t)\n",
    "\n",
    "for key in ('W1', 'B1', 'W2', 'B2'):\n",
    "    print(grad_layer[key] - grad[key])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aff5ec674bd5e7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
