{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd949483bb4959f",
   "metadata": {},
   "source": [
    "## 求导和梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab69081b5c92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4b67c22060109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数fx在x点上的导数，这里用中心差分\n",
    "def numerical_diff(f, x):\n",
    "    # 偏差不能太小 计算机由于精度原因会把太小的值当作0\n",
    "    delta_x = 1e-4 \n",
    "    # 由于delta_x不能无限小，因此用中心差分来减少误差\n",
    "    # dfx/dx = (f(x + delta) - f (x - delta)) / 2h\n",
    "    return (f(x + delta_x) - f(x - delta_x)) / (2 * delta_x)\n",
    "\n",
    "f = lambda x: x ** 2 + 3 * x + 100\n",
    "numerical_diff(f, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a219852cb4499",
   "metadata": {},
   "source": [
    "标量y对向量求导,即为该向量的梯度， 应该需要转置，即行向量的梯度为列向量\n",
    "\n",
    "但是书上没有转制，大概后续乘法会交换顺序把。\n",
    "\n",
    "这里按照书上来，先不转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c240377a7b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x是向量\n",
    "def numerical_gradient(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # 遍历x，分别对每一个参数求导，\n",
    "    # 实际上就是对当前的x[i]进行中心分差计算，其他元素不变\n",
    "    for i in range(x.size):\n",
    "        temp_x_i = x[i]\n",
    "        x[i] = temp_x_i + delta_x\n",
    "        fx1 = f(x)\n",
    "        x[i] = temp_x_i - delta_x\n",
    "        fx2 = f(x)\n",
    "        grad[i] = (fx1 - fx2) / (2 * delta_x)\n",
    "        x[i] = temp_x_i\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# 测试梯度函数\n",
    "f = lambda a: a[0] ** 2 + 2 *a[0] * a[1] + a[2] ** 2\n",
    "numerical_gradient(f, np.array([3.0, 4.0, 5.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbfde840631fe3",
   "metadata": {},
   "source": [
    "接下来可以写出梯度下降法的函数了:x = x - lr * grad_x  \n",
    "x是向量，其中lr是学习率，grad_x是x的梯度  \n",
    "分解开就是:\n",
    "    x[0] = x[0] - lr * grad_x[0]    \n",
    "    x[1] = x[1] - lr * grad_x[1]\n",
    "    ... \n",
    "    x[n] = x[n] - lr * grad_x[n]  \n",
    "重复一定次数即可，求出fx的极小值/最小值了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d542e5f3c77fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降求函数的fx极小值的x坐标，lr系数，学习率 step重复次数\n",
    "def gradient_descent(f, init_x, lr = 0.01, step=100):\n",
    "    x = init_x\n",
    "    for i in range(step):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x = x - lr * grad\n",
    "    return x\n",
    "\n",
    "# 测试下梯度下降\n",
    "fx = lambda x: x[0] ** 2 + x[1] ** 2\n",
    "init_x = np.array([33.0, 20.0])\n",
    "gradient_descent(fx, init_x, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c47cef",
   "metadata": {},
   "source": [
    "定义一个简单的单层网络类，权重w是一个2 * 3的矩阵，偏置先不设置，激活函数是softmax\n",
    "\n",
    "可以计算出这个网络的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72117aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_a = np.exp(x - c) # 减去c是为了防止溢出\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "# 损失函数 交叉熵误差\n",
    "def cross_entropy_error(y, t):\n",
    "    # 由于loge(0)是负无穷大-inf，计算机无法继续之后的运算\n",
    "    # 所以给输入增加一个微小的数，并且不影响结果\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "class SimpleNet:\n",
    "    \n",
    "    # 初始化网络    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = np.random.randn(2, 3)\n",
    "\n",
    "    # 推理一次 即forward    \n",
    "    def predict(self, x):\n",
    "        y = np.dot(x, self.w)\n",
    "        y = softmax(y)\n",
    "        return y\n",
    "\n",
    "    # 网络的损失\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9457c6cd7741770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置np的打印选项：精度最大为小数点后4位，不用科学计数法\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# 简单测试下网络\n",
    "net = SimpleNet()\n",
    "x = np.array([[0.6, 0.9]])\n",
    "t = np.array([0.0, 0.0, 1.0])\n",
    "d = net.loss(x, t)\n",
    "print('loss:', d)\n",
    "\n",
    "\n",
    "# 一个通用的算术求梯度，遍历ndarray进行运算\n",
    "def numerical_gradient_common(f, x):\n",
    "    h = 1e-4 \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # 利用迭代器遍历ndarray，这样才能保证拿到索引并且有修改ndarray的权限\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    for _ in it:  # 也可以这么写while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        temp = x[idx]\n",
    "        x[idx] = temp + h\n",
    "        fx1 = f(x)\n",
    "        x[idx] = temp - h\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * h)\n",
    "        x[idx] = temp\n",
    "    return grad\n",
    "\n",
    "# loss函数输入x t，再通过与w b的计算 可以表示为y = f(w) + f(b)\n",
    "# 因此这里损失对w求导，直接用loss_w即可，求导函数里每次调用f(x)的时候，会按照当前w b x t计算一次\n",
    "# 而我们传入的net.w，进行了微小的更改+h和-h，这里就是损失对w求偏导了\n",
    "# 可以测试一下\n",
    "loss_w = lambda w: net.loss(x, t)\n",
    "grad = numerical_gradient_common(loss_w, net.w)\n",
    "\n",
    "print('grad:', grad)\n",
    "\n",
    "# w[0][0]增加1 看对应的loss增加多少\n",
    "net.w[0][0] += 1\n",
    "d = net.loss(x, t)\n",
    "print('loss:', d)\n",
    "# loss应该是差不多增加了grad[0][0] * w[0][0],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff08242",
   "metadata": {},
   "source": [
    "接下来测试下sgd随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 给一组真实数据\n",
    "true_x = np.array([[0.2, 0.3]])\n",
    "true_w = np.array([[0.3, -0.5, 1.2], [0.2, 1.2, 0.5]])\n",
    "true_y = true_x.dot(true_w)\n",
    "true_y = softmax(true_y)\n",
    "max = np.argmax(true_y)\n",
    "true_y = np.zeros_like(true_y)\n",
    "true_y[0][max] = 1\n",
    "\n",
    "nn = SimpleNet()\n",
    "d = nn.loss(true_x, true_y)\n",
    "print('loss 1:', d)\n",
    "loss_nn = lambda _: nn.loss(true_x, true_y)\n",
    "\n",
    "# sgd \n",
    "lr = 0.1\n",
    "step = 1000\n",
    "for _ in range(step):\n",
    "    grad = numerical_gradient_common(loss_nn, nn.w)\n",
    "    nn.w -= lr * grad\n",
    "    \n",
    "    \n",
    "# 打印sgd之后的误差 和推理结果\n",
    "d = nn.loss(true_x, true_y)\n",
    "print('loss 2:', d)\n",
    "y_hat = nn.predict(true_x)\n",
    "print('y_hat:', y_hat)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e7132be13c578b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
